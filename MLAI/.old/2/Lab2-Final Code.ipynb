{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this lab, you will complete a handwriten digit recognition  number recognition using a basic feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.nn import functional\n",
    "import torchmetrics\n",
    "import statistics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## introduction to tensors\n",
    "\n",
    "Tensors are multi-dimensional arrays with a uniform type, here is an example a 3 3-axis tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t= torch.tensor(\n",
    "[\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9]],\n",
    "  [[10, 11, 12, 13, 14],\n",
    "   [15, 16, 17, 18, 19]],\n",
    "  [[20, 21, 22, 23, 24],\n",
    "   [25, 26, 27, 28, 29]],]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "here is what it looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![image](Resources/tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Try to  understand it with it's shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "you can switch dimension by transpose them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_p = torch.transpose(t, 0, 1)\n",
    "t_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Or fuse several dimension by reshape it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_r = torch.reshape(t, [6, 5])\n",
    "t_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Q:** Please find 2 way to transform the tensor `t` into shape of [10, 3], and state what the different between these two methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"your code here\"\"\"\n",
    "import torch\n",
    "\n",
    "t1 = torch.tensor(\n",
    "    [\n",
    "        [[0, 1, 2, 3, 4],\n",
    "         [5, 6, 7, 8, 9]],\n",
    "        [[10, 11, 12, 13, 14],\n",
    "         [15, 16, 17, 18, 19]],\n",
    "        [[20, 21, 22, 23, 24],\n",
    "         [25, 26, 27, 28, 29]], ]\n",
    ")\n",
    "\n",
    "print(t1.shape)\n",
    "\n",
    "t2 = t1.view(10, 3)\n",
    "print(t2.shape)\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.reshape(t1, [10, 3])\n",
    "print(t3.shape)\n",
    "print(t3)\n",
    "\n",
    "# difference: The main difference is that the view() depends on the memory layout of the original tensor and may not create a continuous tensor. Modify an element in 't_view', which also affects the original tensor 't' because both tensors share the same memory. This is because 'view' does not guarantee that the result is continuous. \n",
    "# On the other hand, whenever possible, reshape() attempts to create a continuous tensor from a copy of the data, so modifying it does not affect the original tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Download and load dataset\n",
    "\n",
    "> MNIST stands for Mixed National Institute of Standards and Technology, which has produced a handwritten digits dataset. This is one of the most researched datasets in machine learning, and is used to classify handwritten digits. This dataset is helpful for predictive analytics because of its sheer size, allowing deep learning to work its magic efficiently. This dataset contains 60,000 training images and 10,000 testing images, formatted as 28 x 28 pixel monochrome images.\n",
    "\n",
    "here we are using the **DataLoader** to build the input data. The Data loader Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
    "It allows us to load data in a small batch at one time, while loading them all at once may make the training process too computationally heavy for your device.\n",
    "\n",
    "[reference for dataloader](https://pytorch.org/docs/stable/data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size_train = 128\n",
    "batch_size_test = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    #load data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data/', train=True, download=True, transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data/', train=False, download=True, transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## illustrate example data\n",
    "\n",
    "here we pick first batch of data, to see what it looks like and its shape when become a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "index, (example_data, example_labels) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot firtst 6 data in in test set\n",
    "ig, axes = plt.subplots(nrows=2, ncols=3)\n",
    "\n",
    "for index_row, row_axes in enumerate(axes):\n",
    "    for index_col, ax in enumerate(row_axes):\n",
    "        i = index_row*3 + index_col\n",
    "        ax.imshow(example_data[i].numpy().squeeze(), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "here we use adjusted standardize for image, in case of the stds become zero. the formula is:\n",
    "\n",
    "$$\n",
    "output = \\frac{x-\\mu}{\\hat{\\sigma}} \\\\\n",
    "\\hat{\\sigma} = max(\\sigma, \\frac{1.0}{\\sqrt{N}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def standardize(data, dim):\n",
    "    means = data.mean(dim = dim, keepdims=True)\n",
    "    stds = data.std(dim = dim, keepdims=True)\n",
    "    stds = torch.maximum(stds, torch.tensor(1./math.sqrt(28*28)))\n",
    "    return (data - means) / stds\n",
    "\n",
    "example_data_standardized = standardize(example_data, dim=(-2, -1))\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(example_data_standardized[0].numpy().flatten(), density=True, label=\"data distribution\")\n",
    "\n",
    "x = np.arange(-3, 3, 0.01)\n",
    "ax.plot(x, norm.pdf(x, 0, 1.), label=\"standard norm distribution\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## building the feed forward neural network\n",
    "\n",
    "there are 5 hidden layers. The first 4 layers use Relu for activation, while the last layer use softmax as input\n",
    "\n",
    "[how to build neural network in pytorch](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n",
    "[category of all pytorch layers(modules)](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "**Q**: why we need softmax as the last activation function?\n",
    "\n",
    "**A**: For multi-classification tasks, the softmax function takes the raw output of the network and converts it into a probability vector that sums to 1, and the sigmoid function can only output values between 0 and 1. In this way, the output can be interpreted as the probability that the input belongs to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10),\n",
    "    nn.Softmax(dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## training the model\n",
    "\n",
    "we use **cross entropy** as loss function and **stochastic gradient descent(SDG)** as optimizer\n",
    "\n",
    "[how to train a pytorch model](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training a model\n",
    "def train(model, n_epochs, learning_rate):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    testing_accuracy = []\n",
    "    average_train_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        highest_accuracy = 0\n",
    "        accuracy_train = torchmetrics.Accuracy(num_classes=10, task=\"multiclass\")\n",
    "        pbar = tqdm(train_loader, desc=\"epoch: \" + str(epoch))\n",
    "        batch_loss = []\n",
    "        for index, (data, label) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = standardize(data, (-2, -1))\n",
    "            x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "            y = model(x)\n",
    "\n",
    "            loss = functional.cross_entropy(y, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acu = accuracy_train(y, label).item()\n",
    "            pbar.set_postfix({\n",
    "                    'batch_accuracy': acu,\n",
    "                    'loss': loss.item()\n",
    "                })\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        #add average loss to the list\n",
    "        average_train_loss.append(statistics.mean(batch_loss))\n",
    "        #test\n",
    "        accuracy_test = torchmetrics.Accuracy(num_classes=10, task=\"multiclass\")\n",
    "\n",
    "        for index, (data, label) in enumerate(test_loader):\n",
    "            x = standardize(data, (-2, -1))\n",
    "            x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "            y = model(x)\n",
    "            accuracy_test.update(y, label)\n",
    "\n",
    "        accu_test = accuracy_test.compute().item()\n",
    "        print('test_accuracy='+str(accu_test), end=', ')\n",
    "\n",
    "        #add test loss to the list\n",
    "        testing_accuracy.append(accu_test)\n",
    "\n",
    "        #save the model of highest accuracy\n",
    "        if accu_test > highest_accuracy:\n",
    "            highest_accuracy = accu_test\n",
    "            torch.save(model.state_dict(), r\"checkpoint\")\n",
    "\n",
    "    return testing_accuracy, average_train_loss\n",
    "\n",
    "accuracy, losses = train(model, n_epochs=20, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation\n",
    "you can check the training history using these two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#draw a plot refecting the change of accuracy and losses relative to the number of epoch\n",
    "print(accuracy)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Q** please draw a plot refecting the change of accuracy and losses relative to the number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"your code here\"\"\"\n",
    "plt.cla()\n",
    "x1 = range(0, 20)\n",
    "print(x1)\n",
    "y1 = losses\n",
    "print(y1)\n",
    "plt.plot(x1, y1, '.-')\n",
    "plt.xlabel('epochs', fontsize=20)\n",
    "plt.ylabel('Train loss', fontsize=20)\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "x2 = range(0, 20)\n",
    "print(x2)\n",
    "y2 = accuracy\n",
    "print(y2)\n",
    "labels = [\"Accuracy\", \"Loss\"]\n",
    "plt.title('Accuracy & Loss vs Epochs', fontsize=20)\n",
    "plt.plot(x2, y2, '.-')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Accuracy & Loss', fontsize=20)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Q** please draw a confusion matrix using test data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read model for evaluation, using test dataloader to draw\n",
    "model.load_state_dict(torch.load(r\"checkpoint\"))\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predicted results and true labels\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Calculate predicted results and true labels using the test dataset\n",
    "for data, label in test_loader:\n",
    "    data = standardize(data, dim=(-2, -1))\n",
    "    data = torch.flatten(data, start_dim=1, end_dim=-1)\n",
    "    output = model(data)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_labels.extend(predicted.tolist())\n",
    "    true_labels.extend(label.tolist())\n",
    "\n",
    "# Calculte the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "plt.matshow(confusion, cmap=plt.cm.Blues)   # Greens, Blues, Oranges, Reds\n",
    "plt.colorbar()\n",
    "for i in range(len(confusion)):\n",
    "    for j in range(len(confusion)):\n",
    "        plt.annotate(confusion[j,i], xy=(i, j), horizontalalignment='center', verticalalignment='center')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# validation\n",
    "\n",
    "**Q** please use different learning rate to train the model, then comparing the training process by draw a similar plot mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"your code here\"\"\"\n",
    "# we just change the accuracy, losses = train(model, n_epochs=20, learning_rate=0.1) to learning_rate=0.02 and learning_rate=0.5 respectively.\n",
    "\n",
    "accuracy, losses = train(model, n_epochs=20, learning_rate=0.1)\n",
    "accuracy, losses = train(model, n_epochs=20, learning_rate=0.5)\n",
    "accuracy, losses = train(model, n_epochs=20, learning_rate=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
